{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before this, run the following: docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "REDIS_URL = \"redis://localhost:6379/0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model='llama-3.3-70b-versatile'\n",
    ")\n",
    "\n",
    "system = \"\"\"\n",
    "You are an investing professional, specializing in stock-picking. Answer the user's questions on investing in detail.\n",
    "Given a chat history and the latest user question which might reference context in the chat history, answer the question\n",
    "to the best of your ability. If you do not know the answer, just return \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "def get_message_history(session_id: str) -> RedisChatMessageHistory:\n",
    "    return RedisChatMessageHistory(session_id, url=REDIS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"No, you didn't ask me about cosine before. This conversation just started, and your first question was about whether you asked me about cosine before.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 170, 'total_tokens': 201, 'completion_time': 0.112727273, 'prompt_time': 0.010632587, 'queue_time': 0.233271133, 'total_time': 0.12335986}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_76dc6cf67d', 'finish_reason': 'stop', 'logprobs': None}, id='run-bd6c7f43-c4ee-426b-b20b-dc0eaada4ede-0', usage_metadata={'input_tokens': 170, 'output_tokens': 31, 'total_tokens': 201})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"investing\", \"input\": \"Did I ask you about cosine before?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = get_message_history('foobar')\n",
    "history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  history: RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "}), kwargs={}, config={'run_name': 'insert_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_message_history at 0x00000230E1036520>, input_messages_key='input', history_messages_key='history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into a class\n",
    "\n",
    "class InvestingChatBot:\n",
    "\n",
    "    # define default system prompt\n",
    "    system_prompt = \"\"\"\n",
    "                    You are an investing professional, specializing in stock-picking. Answer the user's questions on investing in detail.\n",
    "                    Given a chat history and the latest user question which might reference context in the chat history, answer the question\n",
    "                    to the best of your ability. If you do not know the answer, just return \"I don't know\".\n",
    "                    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str, temperature: float, session_id: str):\n",
    "\n",
    "        '''Initialize the model, temperature, and the session id'''\n",
    "\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.session_id = session_id\n",
    "\n",
    "        self.chat = self.create_llm_chain_with_history()\n",
    "        \n",
    "\n",
    "    def get_message_history(self) -> RedisChatMessageHistory:\n",
    "        '''\n",
    "        Get the message history of the current session\n",
    "        '''\n",
    "        return RedisChatMessageHistory(self.session_id, url=REDIS_URL)\n",
    "    \n",
    "    def clear_history(self):\n",
    "        '''\n",
    "        Clears the message history for the current session\n",
    "        '''\n",
    "        history = self.get_message_history()\n",
    "        history.clear()\n",
    "    \n",
    "    def create_llm_chain_with_history(self) -> RunnableWithMessageHistory:\n",
    "        '''\n",
    "        Create a runnable with message history. Message history saved to redis database for persistence\n",
    "        '''\n",
    "\n",
    "        chat = ChatGroq(\n",
    "            temperature=self.temperature,\n",
    "            model=self.model\n",
    "        )\n",
    "\n",
    "        # define the prompt\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.system_prompt),\n",
    "                MessagesPlaceholder(variable_name=\"history\"), # add in chat history\n",
    "                (\"human\", \"{input}\")\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # create the chain\n",
    "        chain = prompt | chat\n",
    "\n",
    "        # add message history to the llm\n",
    "        chain_with_message_history = RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            self.get_message_history, # add in method to get message history\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "\n",
    "        return chain_with_message_history\n",
    "    \n",
    "    def prompt(self, ability: str, input: str) -> str:\n",
    "        '''\n",
    "        Prompt the LLM, get a response in string\n",
    "        '''\n",
    "        \n",
    "        response = self.chat.invoke(\n",
    "            {\"ability\": ability, \"input\": input},\n",
    "            config={\"configurable\": {\"session_id\": self.session_id}},\n",
    "        )\n",
    "\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n",
      "I don't know. This conversation just started, and I haven't received any information about your name. If you'd like to share it with me, I'd be happy to know, but for now, I don't have that information. By the way, are you interested in discussing investing or stock-picking strategies?\n"
     ]
    }
   ],
   "source": [
    "investingChatBot = InvestingChatBot(\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    0,\n",
    "    'jiahao'\n",
    ")\n",
    "\n",
    "r2 = investingChatBot.prompt(\n",
    "    ability='investing',\n",
    "    input='What is my name?'\n",
    ")\n",
    "\n",
    "print(r2)\n",
    "\n",
    "# investingChatBot.clear_history()\n",
    "\n",
    "r3 = investingChatBot.prompt(\n",
    "    ability='investing',\n",
    "    input='What is my name?'\n",
    ")\n",
    "\n",
    "print(r3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
